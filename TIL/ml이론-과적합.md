### 과적합에 대하여

[원문 링크](https://towardsdatascience.com/is-your-model-overfitting-or-maybe-underfitting-an-example-using-a-neural-network-in-python-4faf155398d2)

#### 1. Introduction

대부분에 경우에서 ML(Machine Learnig) 모델의 퍼포먼스가 좋지 못한 것은 과적합 혹은 과소적합 때문이다. 모델이 제성능을 내기 위해서는 일반화가 되어야한다. 

#### 2. 일반화

ML에서의 일반화란 룰/패턴/기능을 모델에 학습시키고, 모델이 학습할때 볼 수 없었던 특정 예시에 얼마나 잘 적용할 수 있는가를 의미한다. 이를 unseen set 이나 test set이라 부른다.

좋은 ML의 목표는 훈련 데이터에서부터 문제가 가지고 있는 어떤 데이터가 와도 일반화가 가능한 것이다. 이는 모델이 한번도 보지 못한 데이터(예를들면 미래에 생성될 데이터)를 예측할 수 있게 해준다.

#### 3. 과적합이란

*과적합은 ML모델이 훈련 데이터에 과도하게 학습된 것을 의미한다.*

과적합은 모델이 데이터를 학습할 때 훈련 데이터 뿐만 아니라 그 안에 들어있는 노이즈도 같이 학습하여 새로운 데이터에 대한 모델에 성능에 부정적인 영향을 미치는 것을 의미한다.

모델은 훈련 데이터에 있는 노이즈(무작위 변동)를 룰/패턴으로 학습한다. 그러나, 노이즈는 unseen data에 대해 적용할 수 없고, 정확도, mse, mae와 같은 모델의 성능에 부정적 영향을 미친다.

#### 4. 과소적합이란

*과소적합은 ML모델이 훈련데이터나 새로운 데이터에 대해서 일반화할 수 없는 것을 의미한다.*

모델이 과소적합되면 훈련 데이터에 대해서 좋지 못한 성능을 보인다. 그러면 훈련데이터와 테스트데이터에 대한 모델의 에러가 아주 높게 측정될 것이다.

#### 5. sweet spot and good model

*과적합과 과소적합 사이의 적정점을 찾아야 좋은 모델이 될 수 있다.*

적정점이란 모델의 에러가 훈련데이터와 테스트데이터에 대해서 모두 낮은 지점을 말한다. 다시말해, 적정점은 테스트데이터에 대한 에러가 증가하기 시작하는 시점에서 훈련데이터에 대한 에러가 적절하게 낮은 지점을 의미한다.

#### 6. 과적합을 제한하는 방법

1. cv(cross-validation) 사용
2. 모델의 복잡도를 낮추는 방법

1번에서는 k-fold cv를 사용하는게 예시가 될 수 있으며, 2번에서는 svm모델에서 RBF kernel을 linear kernel로 바꾸는 것이 예시가 될 수 있다.

#### 7. 과소적합을 제한하는 방법

1. 다른 모델을 시도해본다
2. 현재 모델의 복잡도를 높인다.

#### 8. 결론

과적합과 과소적합은 ML 모델의 좋지 못한 성능의 가장 흔한 원인이다. 그리고 과적합이 과소적합보다 더 자주 발생한다.

+ 과적합은 훈련 데이터에 대한 에러는 낮지만 테스트 데이터에 대한 에러는 큰 것을 의미한다.
+ 과소적합은 훈련 데이터와 테스트 데이터에 대한 에러가 둘 다 높은 것을 의미한다.

이 문제에 대한 해결법은 보통 cv가 사용된다.
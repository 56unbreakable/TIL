{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n",
      "Downloading: 100%|██████████| 473/473 [00:00<00:00, 237kB/s]\n",
      "Downloading: 100%|██████████| 249M/249M [00:14<00:00, 18.1MB/s] \n",
      "All model checkpoint layers were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the layers of TFDistilBertForQuestionAnswering were initialized from the model checkpoint at distilbert-base-cased-distilled-squad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n",
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 5.84kB/s]\n",
      "Downloading: 100%|██████████| 208k/208k [00:00<00:00, 362kB/s]  \n",
      "Downloading: 100%|██████████| 426k/426k [00:00<00:00, 442kB/s]  \n"
     ]
    }
   ],
   "source": [
    "qa = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Seoul, officially the Seoul Special City, is the capital and largest metropolis of South Korea. \n",
    "Seoul has a population of 9.7 million people, \n",
    "and forms the heart of the Seoul Capital Area with the surrounding Incheon metropolis and Gyeonggi province.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.8629825115203857, 'start': 1, 'end': 6, 'answer': 'Seoul'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(question = \"where is the capital city of South Korea?\",context = text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9299280643463135,\n",
       " 'start': 124,\n",
       " 'end': 135,\n",
       " 'answer': '9.7 million'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(question = \"How many people live in Seoul?\",context = text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\n",
      "Downloading: 100%|██████████| 998/998 [00:00<00:00, 999kB/s]\n",
      "Downloading: 100%|██████████| 1.24G/1.24G [01:15<00:00, 17.7MB/s]\n",
      "Some layers from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing TFBertForTokenClassification: ['dropout_147']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForTokenClassification were initialized from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n",
      "Downloading: 100%|██████████| 60.0/60.0 [00:00<00:00, 15.0kB/s]\n",
      "Downloading: 100%|██████████| 208k/208k [00:00<00:00, 346kB/s]  \n"
     ]
    }
   ],
   "source": [
    "ner = pipeline(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-LOC',\n",
       "  'score': 0.9995671,\n",
       "  'index': 1,\n",
       "  'word': 'Seoul',\n",
       "  'start': 1,\n",
       "  'end': 6},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9981558,\n",
       "  'index': 5,\n",
       "  'word': 'Seoul',\n",
       "  'start': 23,\n",
       "  'end': 28},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.97723025,\n",
       "  'index': 6,\n",
       "  'word': 'Special',\n",
       "  'start': 29,\n",
       "  'end': 36},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9895074,\n",
       "  'index': 7,\n",
       "  'word': 'City',\n",
       "  'start': 37,\n",
       "  'end': 41},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9969215,\n",
       "  'index': 17,\n",
       "  'word': 'South',\n",
       "  'start': 84,\n",
       "  'end': 89},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9992411,\n",
       "  'index': 18,\n",
       "  'word': 'Korea',\n",
       "  'start': 90,\n",
       "  'end': 95},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.999453,\n",
       "  'index': 20,\n",
       "  'word': 'Seoul',\n",
       "  'start': 98,\n",
       "  'end': 103},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.98211795,\n",
       "  'index': 37,\n",
       "  'word': 'Seoul',\n",
       "  'start': 172,\n",
       "  'end': 177},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9603083,\n",
       "  'index': 38,\n",
       "  'word': 'Capital',\n",
       "  'start': 178,\n",
       "  'end': 185},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9373543,\n",
       "  'index': 39,\n",
       "  'word': 'Area',\n",
       "  'start': 186,\n",
       "  'end': 190},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.998509,\n",
       "  'index': 43,\n",
       "  'word': 'Inc',\n",
       "  'start': 212,\n",
       "  'end': 215},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.97816354,\n",
       "  'index': 44,\n",
       "  'word': '##he',\n",
       "  'start': 215,\n",
       "  'end': 217},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9987452,\n",
       "  'index': 45,\n",
       "  'word': '##on',\n",
       "  'start': 217,\n",
       "  'end': 219},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99873143,\n",
       "  'index': 49,\n",
       "  'word': 'G',\n",
       "  'start': 235,\n",
       "  'end': 236},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.991698,\n",
       "  'index': 50,\n",
       "  'word': '##ye',\n",
       "  'start': 236,\n",
       "  'end': 238},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.9972524,\n",
       "  'index': 51,\n",
       "  'word': '##ong',\n",
       "  'start': 238,\n",
       "  'end': 241},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': 0.99624276,\n",
       "  'index': 52,\n",
       "  'word': '##gi',\n",
       "  'start': 241,\n",
       "  'end': 243}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-small (https://huggingface.co/t5-small)\n",
      "Downloading: 100%|██████████| 1.17k/1.17k [00:00<00:00, 602kB/s]\n",
      "Downloading: 100%|██████████| 231M/231M [00:14<00:00, 16.4MB/s] \n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "Downloading: 100%|██████████| 773k/773k [00:01<00:00, 708kB/s]  \n",
      "Downloading: 100%|██████████| 1.32M/1.32M [00:02<00:00, 513kB/s] \n"
     ]
    }
   ],
   "source": [
    "summ = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "A zettelkasten consists of many individual notes with ideas and other short pieces of information that are taken down as they occur or are acquired.\n",
    "The notes may be numbered hierarchically so that new notes may be inserted at the appropriate place, \n",
    "and contain metadata to allow the note-taker to associate notes with each other. \n",
    "For example, notes may contain subject headings or tags that describe key aspects of the note, and they may reference other notes. \n",
    "The numbering, metadata, format and structure of the notes is subject to variation depending on the specific method employed.\n",
    "A zettelkasten may be created and used in a digital format, sometimes using personal knowledge management software. \n",
    "But it can be and has long been done on paper using index cards.\n",
    "The system not only allows a researcher to store and retrieve information related to their research, but has also been used to enhance creativity.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 200, but you input_length is only 190. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=95)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'zettelkasten consists of many individual notes with ideas and other short pieces of information that are taken down as they occur or are acquired . notes may contain subject headings or tags that describe key aspects of the note, and they may reference other notes .'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summ(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to roberta-large-mnli (https://huggingface.co/roberta-large-mnli)\n",
      "Downloading: 100%|██████████| 688/688 [00:00<00:00, 671kB/s]\n",
      "Downloading: 100%|██████████| 1.33G/1.33G [01:19<00:00, 17.9MB/s]\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at roberta-large-mnli.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "Downloading: 100%|██████████| 878k/878k [00:01<00:00, 543kB/s]  \n",
      "Downloading: 100%|██████████| 446k/446k [00:00<00:00, 459kB/s]  \n",
      "Downloading: 100%|██████████| 1.29M/1.29M [00:02<00:00, 602kB/s] \n"
     ]
    }
   ],
   "source": [
    "zs = pipeline(\"zero-shot-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = \"pizza is my favorite food\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = [\"food\",\"ocean\",\"space\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'pizza is my favorite food',\n",
       " 'labels': ['food', 'space', 'ocean'],\n",
       " 'scores': [0.9861140847206116, 0.007536611519753933, 0.006349377799779177]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zs(seq,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'pizza is my favorite food',\n",
       " 'labels': ['life', 'society', 'politics'],\n",
       " 'scores': [0.5483356714248657, 0.32130083441734314, 0.13036344945430756]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = [\"politics\",\"society\",\"life\"]\n",
    "zs(seq,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83d3e1d533d06a5ed13bdd408cfdbe5752516a627e355ce33664633b6ff4534e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
